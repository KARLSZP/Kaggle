{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import torch\n",
    "import torchtext\n",
    "import pickle as pk\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "\n",
    "\n",
    "NGRAMS = 2\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "_types = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "          'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "type_dict = {_type:i for i, _type in enumerate(_types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader(file_path, ngrams, yield_label=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(file_path, encoding=\"utf8\") as file:\n",
    "        reader = unicode_csv_reader(file)\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            tokens = ' '.join(line[1:])\n",
    "            tokens = tokenizer(tokens)            \n",
    "            if yield_label:\n",
    "                yield type_dict[line[0]], ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "                \n",
    "\n",
    "def build_data(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    # Use tqdm to show building speed\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for label, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not '<unk>'),\n",
    "                                 [vocab[token] for token in tokens])\n",
    "                tokens = torch.tensor(token_ids)\n",
    "                \n",
    "            # Update data and labels\n",
    "            data.append((label, tokens))\n",
    "            labels.append(label)\n",
    "            t.update(1)\n",
    "    return data, set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class postsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, data, labels):\n",
    "        super(postsDataset, self).__init__()\n",
    "        self._vocab = vocab\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        return self._data[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self._data:\n",
    "            yield x\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab\n",
    "    \n",
    "\n",
    "def build_dataset(train_name, ngrams, include_unk=True):\n",
    "    # Build vocabulary\n",
    "    vocab = build_vocab_from_iterator(csv_reader(train_name, ngrams))\n",
    "    \n",
    "    # Build train (data, label) tuples\n",
    "    train_data, train_labels = build_data(\n",
    "        vocab,\n",
    "        csv_reader(train_name, ngrams, yield_label=True),\n",
    "        include_unk)\n",
    "    \n",
    "    return postsDataset(vocab, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8675lines [00:10, 789.03lines/s]\n",
      "8675lines [00:17, 482.18lines/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = build_dataset('pickles/sep_mbti.csv', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MbtiClf(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_data.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_data.get_labels())\n",
    "model = MbtiClf(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MbtiClf(\n",
       "  (embedding): EmbeddingBag(2113142, 32, mode=mean)\n",
       "  (fc): Linear(in_features=32, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.1473(train)\t|\tAcc: 19.0%(train)\n",
      "\tLoss: 0.0101(valid)\t|\tAcc: 0.9%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.1267(train)\t|\tAcc: 34.4%(train)\n",
      "\tLoss: 0.0056(valid)\t|\tAcc: 23.0%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.1064(train)\t|\tAcc: 50.3%(train)\n",
      "\tLoss: 0.0045(valid)\t|\tAcc: 38.9%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0954(train)\t|\tAcc: 58.0%(train)\n",
      "\tLoss: 0.0054(valid)\t|\tAcc: 38.2%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0898(train)\t|\tAcc: 61.3%(train)\n",
      "\tLoss: 0.0042(valid)\t|\tAcc: 52.8%(valid)\n",
      "Epoch: 25  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0864(train)\t|\tAcc: 63.2%(train)\n",
      "\tLoss: 0.0032(valid)\t|\tAcc: 52.1%(valid)\n",
      "Epoch: 30  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0844(train)\t|\tAcc: 64.0%(train)\n",
      "\tLoss: 0.0041(valid)\t|\tAcc: 52.3%(valid)\n",
      "Epoch: 35  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0834(train)\t|\tAcc: 64.8%(train)\n",
      "\tLoss: 0.0044(valid)\t|\tAcc: 53.9%(valid)\n",
      "Epoch: 40  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0831(train)\t|\tAcc: 65.0%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 52.8%(valid)\n",
      "Epoch: 45  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0823(train)\t|\tAcc: 65.1%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 54.1%(valid)\n",
      "Epoch: 50  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0822(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0042(valid)\t|\tAcc: 53.5%(valid)\n",
      "Epoch: 55  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0821(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0042(valid)\t|\tAcc: 54.1%(valid)\n",
      "Epoch: 60  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0822(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 53.9%(valid)\n",
      "Epoch: 65  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0818(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 53.9%(valid)\n",
      "Epoch: 70  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0818(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 54.1%(valid)\n",
      "Epoch: 75  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0818(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 54.1%(valid)\n",
      "Epoch: 80  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0821(train)\t|\tAcc: 65.4%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 54.1%(valid)\n",
      "Epoch: 85  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0820(train)\t|\tAcc: 65.4%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 54.1%(valid)\n",
      "Epoch: 90  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0819(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 54.1%(valid)\n",
      "Epoch: 95  | time in 0 minutes, 1 seconds\n",
      "\tLoss: 0.0819(train)\t|\tAcc: 65.3%(train)\n",
      "\tLoss: 0.0043(valid)\t|\tAcc: 54.1%(valid)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_EPOCHS = 100\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_data) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_data, [train_len, len(train_data) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch: %d' %(epoch), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "        print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "        print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Checking the results of test dataset...')\n",
    "# test_loss, test_acc = test(test_dataset)\n",
    "# print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: INTJ\n",
      "1.具强大动力与本意来达成目的与创意—固执顽固者\n",
      "2.有宏大的愿景且能快速在众多外界事件中找出有意义的模范\n",
      "3.对所承负职务，具良好能力于策划工作并完成\n",
      "4.具怀疑心、挑剔性、独立性、果决，对专业水准及绩效要求高\n"
     ]
    }
   ],
   "source": [
    "with open('pickles/type_explanation.pk', 'rb') as pkl:\n",
    "    type_explanation = pk.load(pkl)\n",
    "    \n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "test_post = \"Can’t believe how rude this guy is busting shapes during Boris’ speech smh\"\n",
    "\n",
    "vocab = train_data.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "predict_type = _types[predict(test_post, model, vocab, 2)]\n",
    "\n",
    "print(\"Type: %s\" % predict_type)\n",
    "for i in type_explanation[predict_type]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
